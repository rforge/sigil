%% This is an automatically generated bibliography file.
%% Please do not edit this file (update it with 'bibextract').

@article{Artstein:Poesio:08,
	Author = {Artstein, Ron and Poesio, Massimo},
	Date-Added = {2009-02-26 15:03:37 +0100},
	Date-Modified = {2009-02-26 15:05:54 +0100},
	Journal = {Computational Linguistics},
	Keywords = {IAA},
	Number = {4},
	Pages = {555--596},
	Title = {Survey Article: Inter-Coder Agreement for Computational Linguistics},
	Volume = {34},
	Year = {2008},
	Abstract = {This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks---but that their use makes the interpretation of the value of the coefficient even harder.
}}


@article{Bennett:Alpert:Goldstein:54,
	Author = {Bennett, E. M. and Alpert, R. and Goldstein, A. C.},
	Date-Added = {2009-07-28 01:28:01 +0200},
	Date-Modified = {2009-07-28 01:30:10 +0200},
	Journal = {Public Opinion Quarterly},
	Keywords = {IAA},
	Number = {3},
	Pages = {303--308},
	Title = {Communications through limited questioning},
	Volume = {18},
	Year = {1954}}

@inproceedings{Brants:00a,
	Address = {Athens, Greece},
	Author = {Brants, Thorsten},
	Booktitle = {Proceedings of the Second International Conference on Language Resources and Evaluation (LREC 2000)},
	Date-Added = {2007-02-26 21:19:54 +0100},
	Date-Modified = {2008-01-13 23:31:50 +0100},
	Keywords = {tagging; IAA; parsing},
	Title = {Inter-Annotator Agreement for a {German} Newspaper Corpus},
	Year = {2000},
	Annote = {Detailed study of inter-annotator agreement on the NEGRA corpus.  Excellent reference for reliability of manually annotated POS tags and syntactic analyses.

POS-Tagging: observed agreement of 98.57% between annotators (on 147k tokens), no kappa values given.  Final consensus annotation after ``discussions and several iterations of cleaning'' has 98.8% agreement with each annotator (before discussion).  My conclusion: accuracy above 98.5% cannot be expected from automatic POS tagger and will probably reflect overtraining on remaining errors in gold standard.

Syntactic annotation: F-score for matching constituents between annotators are around 93% (labelled and unlabelled are close together, but down to 88.5% when edge labels are taken into account).  Final annotation has 95%-96% F-score compared to individual annotators.}}

@article{Carletta:96,
	Author = {Carletta, Jean},
	Journal = {Computational Linguistics},
	Number = 2,
	Pages = {249--254},
	Source = {Stefan},
	Title = {Assessing Agreement on Classification Tasks: the Kappa Statistic},
	Volume = 22,
	Year = 1996}

@article{Cohen:60,
	Author = {Cohen, Jacob},
	Date-Modified = {2009-07-28 01:31:38 +0200},
	Journal = {Educational and Psychological Measurement},
	Keywords = {IAA},
	Pages = {37--46},
	Source = {Stefan (copy from Bettina)},
	Title = {A Coefficient of Agreement for Nominal Scales},
	Volume = 20,
	Year = 1960}

@article{DiEugenio:Glass:04,
	Author = {Di Eugenio, Barbara and Glass, Michael},
	Date-Modified = {2008-01-13 23:29:59 +0100},
	Journal = {Computational Linguistics},
	Keywords = {IAA},
	Number = 1,
	Pages = {95--101},
	Title = {The Kappa Statistic: A Second Look},
	Volume = 30,
	Year = 2004}

@article{Fleiss:Cohen:Everitt:69,
	Author = {Fleiss, Joseph L. and Cohen, Jacob and Everitt, B. S.},
	Date-Modified = {2007-06-05 21:45:12 +0200},
	Journal = {Psychological Bulletin},
	Keywords = {IAA},
	Number = 5,
	Pages = {323--327},
	Source = {Stefan (copy from Brigitte)},
	Title = {Large Sample Standard Errors of Kappa and Weighted Kappa},
	Volume = 72,
	Year = 1969}

@inproceedings{Green:97,
	Address = {San Diego, CA},
	Author = {Green, Annette M.},
	Booktitle = {Proceedings of the Twenty-Second Annual SAS Users Group International Conference (online)},
	Date-Added = {2009-07-28 08:52:12 +0200},
	Date-Modified = {2009-07-28 08:52:26 +0200},
	Keywords = {IAA},
	Month = {March},
	Pdf = {http://www2.sas.com/proceedings/sugi22/POSTERS/PAPER241.PDF},
	Title = {Kappa Statistics for Multiple Raters Using Categorical Classifications},
	Year = {1997}}

@inproceedings{Krenn:Evert:Zinsmeister:04,
	Address = {Vienna, Austria},
	Author = {Krenn, Brigitte and Evert, Stefan and Zinsmeister, Heike},
	Booktitle = {Proceedings of KONVENS 2004},
	Date-Modified = {2007-11-06 11:34:28 +0100},
	Keywords = {IAA},
	Pages = {89--96},
	Title = {Determining Intercoder Agreement for a Collocation Identification Task},
	Year = 2004}

@book{Krippendorff:80,
	Address = {Beverly Hills, CA},
	Author = {Krippendorff, Klaus},
	Publisher = {Sage Publications},
	Source = {cited from DiEugenio:Glass:04},
	Title = {Content Analysis: An Introduction to Its Methodology},
	Year = 1980}

@article{Landis:Koch:77,
	Author = {Landis, J. Richard and Koch, Gary G.},
	Date-Added = {2009-07-28 08:35:05 +0200},
	Date-Modified = {2009-07-28 08:52:39 +0200},
	Journal = {Biometrics},
	Keywords = {IAA},
	Number = {1},
	Pages = {159--174},
	Title = {The measurement of observer agreement for categorical data},
	Volume = {33},
	Year = {1977}}

@article{Lee:Tu:94,
	Author = {Lee, J. Jack and Tu, Z. Nora},
	Date-Added = {2009-02-26 15:14:51 +0100},
	Date-Modified = {2009-02-26 15:16:01 +0100},
	Journal = {Journal of Computational and Graphical Statistics},
	Keywords = {IAA},
	Number = {3},
	Pages = {301--321},
	Publisher = {American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of America},
	Title = {A Better Confidence Interval for Kappa ($\kappa$) on Measuring Agreement between Two Raters with Binary Outcomes},
	Url = {http://www.jstor.org/stable/1390914},
	Volume = {3},
	Year = {1994},
	Abstract = {Although the kappa statistic is widely used in measuring interrater agreement, it is known that the standard confidence interval estimation behaves poorly in small samples and for nonzero kappas. Efforts have been made to improve the estimation through transformation and Edgeworth expansion (Flack 1987). The results remain unsatisfactory when kappa is far from 0, however, even with the sample size as large as 100. In this article we reparameterize the kappa statistic to reveal its relationship with the marginal probability of agreement. The reparameterization not only gives a more meaningful interpretation of kappa but also clearly demonstrates that the range of kappa depends on the marginal probabilities. Various two- and three-dimensional plots are shown to illustrate the relationship among these parameters. The immediate application is to construct a new confidence interval based on the profile variance and reparameterization. Extensive simulation studies show that the new confidence interval performs very well in almost all parameter settings even when other methods fail.}}


@article{Scott:55,
	Author = {Scott, William A.},
	Date-Added = {2009-07-28 01:30:40 +0200},
	Date-Modified = {2009-07-28 01:31:32 +0200},
	Journal = {Public Opinion Quarterly},
	Keywords = {IAA},
	Number = {3},
	Pages = {321--325},
	Title = {Reliability of Content Analysis: The case of nominal scale coding},
	Volume = {19},
	Year = {1955}}

@inproceedings{Veronis:98,
	Address = {Herstmonceux Castle, Sussex, UK},
	Author = {V{\'e}ronis, Jean},
	Booktitle = {Proceedings of {SENSEVAL-1}},
	Date-Added = {2009-07-28 00:04:07 +0200},
	Date-Modified = {2009-07-28 00:05:38 +0200},
	Keywords = {IAA},
	Title = {A study of polysemy judgements and inter-annotator agreement},
	Url = {http://www.itri.brighton.ac.uk/events/senseval/ARCHIVE/PROCEEDINGS/},
	Year = {1998}}
