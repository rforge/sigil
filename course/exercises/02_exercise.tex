\documentclass[a4paper,12pt]{article}

\input{style.tex}

\begin{document}

\emph{Marco Baroni \& Stefan Evert} \hfill %
{\small \url{http://SIGIL.R-Forge.R-Project.org/}}

\begin{center}
  \textbf{\large Statistical Analysis of Corpus Data with R}

  \textbf{\large --- Exercise Sheet for Unit \#2 ---}
\end{center}


In this exercise, your task is to extract frequency information from the British National Corpus using the \emph{BNCweb} interface (Hoffmann
\emph{et al.} 2008) and to perform statistical frequency comparisons for these data in R.

Participants of a SIGIL course will be given access to a BNCweb server at
\begin{center}
  \url{https://corpora.linguistik.uni-erlangen.de/bncweb/}
\end{center}
Other people can use a public demo server at \url{http://bncweb.lancs.ac.uk/}
after applying for a free account (\url{http://bncweb.lancs.ac.uk/bncwebSignup}).


\begin{enumerate}
\item Log into the BNCweb server and familiarise yourself with the Web interface and its Simple Query Syntax (CEQL).  
  Learn how to search for word forms, lemmata and phrases, as well as for lexico-grammatical patterns (optional).
\item Pick a word, phrase or grammatical pattern of interest, and calculate its distribution across text types
  (or other metadata categories).  You can either note down the resulting counts and enter them manually into R,
  or copy and paste the distribution table displayed by BNCweb into a text editor or spreadsheet software.%
  \footnote{Users of Microsoft Excel should make sure to paste the table as plain text rather than HTML.}
\item The BNCweb distribution table includes total word counts for each category.  Are word tokens a sensible unit
  of measurement?  If not, use a second query to obtain suitable by-category totals and combine them with the frequency counts from above.%
  \footnote{You may not be able to carry out this step because of data size limitations for your BNCweb account.}
\item Perform frequency comparison tests for various pairs of categories.  Which differences are significant?  
  Do you think that their effect size makes them linguistically relevant?
\item If you perform pairwise frequency comparisons for all text types, you
  will have to carry out 28 hypothesis tests in total.  What could be a
  fundamental problem of such an approach (apart from being extremely
  tedious)?
\item The R functions \texttt{fisher.test()} and \texttt{chisq.test()} can
  also be applied to a $2\times n$ contingency table in order to compare all
  $n$ categories at once.  Construct such a table from your data, e.g.\ using
  \texttt{rbind()} to combine two row vectors.  Is there a significant difference 
  between your categories?  What exactly is the null hypothesis of this test?
\item The phrase \verb_{click/V} on_ (CEQL query) is significantly more
  frequent in ``other published material'' than any other text type.  Can you
  think of a possible explanation for this observation?  You might want to
  take a closer look at the dispersion count (number of different texts) and
  some corpus examples.
\end{enumerate}

\subsubsection*{References}

Hoffmann, Sebastian; Evert, Stefan; Smith, Nicholas; Lee, David; Berglund~Prytz, Ylva (2008).
{\em Corpus Linguistics with {BNCweb} -- a Practical Guide}, volume~6 of {\em English Corpus Linguistics}.
Peter Lang, Frankfurt am Main.

\end{document}
